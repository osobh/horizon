//! GPU Agents Benchmark Suite Launcher
//!
//! This binary provides a comprehensive benchmarking tool for GPU agents,
//! allowing users to test scalability, LLM integration, knowledge graphs,
//! and evolution strategies with configurable parameters.

use anyhow::{Context, Result};
use chrono::{DateTime, Local};
use clap::{Arg, ArgMatches, Command};
use gpu_agents::benchmarks::{BenchmarkPhase, ProgressWriter};
use serde::{Deserialize, Serialize};
use serde_yaml;
use std::path::{Path, PathBuf};
use std::time::Instant;
use toml;

// Import GPU benchmark functions will be added when implemented

#[tokio::main]
async fn main() -> Result<()> {
    let matches = Command::new("gpu-agents-benchmark")
        .version("1.0.0")
        .author("GPU Agents Team")
        .about("Comprehensive benchmark suite for GPU agents performance testing")
        .arg(
            Arg::new("config")
                .short('c')
                .long("config")
                .value_name("FILE")
                .help("Path to benchmark configuration file"),
        )
        .arg(
            Arg::new("output")
                .short('o')
                .long("output")
                .value_name("DIR")
                .help("Output directory for benchmark results")
                .default_value("reports"),
        )
        .arg(
            Arg::new("quick")
                .short('q')
                .long("quick")
                .help("Run quick benchmark tests (reduced scope)")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("stress")
                .short('s')
                .long("stress")
                .help("Run stress benchmark tests (maximum scope)")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("scalability-only")
                .long("scalability-only")
                .help("Run only scalability benchmarks")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("llm-only")
                .long("llm-only")
                .help("Run only LLM integration benchmarks")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("knowledge-graph-only")
                .long("knowledge-graph-only")
                .help("Run only knowledge graph benchmarks")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("evolution-only")
                .long("evolution-only")
                .help("Run only evolution strategy benchmarks")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("gpu-evolution-only")
                .long("gpu-evolution-only")
                .help("Run only GPU evolution benchmarks")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("gpu-knowledge-only")
                .long("gpu-knowledge-only")
                .help("Run only GPU knowledge graph benchmarks")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("gpu-streaming-only")
                .long("gpu-streaming-only")
                .help("Run only GPU streaming benchmarks")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("gpu-all")
                .long("gpu-all")
                .help("Run all GPU benchmarks")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("include-gpu")
                .long("include-gpu")
                .help("Include GPU benchmarks in the full suite")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("no-reports")
                .long("no-reports")
                .help("Skip report generation (only output JSON)")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("verbose")
                .short('v')
                .long("verbose")
                .help("Enable verbose output")
                .action(clap::ArgAction::SetTrue),
        )
        .arg(
            Arg::new("scenario")
                .long("scenario")
                .value_name("FILE")
                .help("Run a specific scenario from YAML/TOML file")
                .conflicts_with_all(&[
                    "scalability-only",
                    "llm-only",
                    "knowledge-graph-only",
                    "evolution-only",
                ]),
        )
        .arg(
            Arg::new("suite")
                .long("suite")
                .value_name("DIR")
                .help("Run all scenarios in a directory")
                .conflicts_with_all(&[
                    "scenario",
                    "scalability-only",
                    "llm-only",
                    "knowledge-graph-only",
                    "evolution-only",
                ]),
        )
        .get_matches();

    // Initialize logging
    if matches.get_flag("verbose") {
        env_logger::Builder::from_env(env_logger::Env::default().default_filter_or("debug")).init();
    } else {
        env_logger::Builder::from_env(env_logger::Env::default().default_filter_or("info")).init();
    }

    println!("üöÄ GPU Agents Benchmark Suite");
    println!("==============================");

    // Create timestamped output directory first
    let base_output_dir = PathBuf::from(matches.get_one::<String>("output").unwrap());
    let output_dir = create_timestamped_output_dir(&base_output_dir)?;

    // Initialize progress writer in timestamped directory logs subfolder
    let log_path = output_dir.join("logs").join("benchmark_progress.log");
    let progress_writer = ProgressWriter::new(&log_path.to_string_lossy())?;
    progress_writer.log_phase(BenchmarkPhase::Initialization)?;

    let start_time = Instant::now();

    // Parse configuration
    let benchmark_config = load_benchmark_config(&matches)?;

    println!("üìÅ Output directory: {}", output_dir.display());

    // Handle scenario-based benchmarks
    let results = if let Some(scenario_path) = matches.get_one::<String>("scenario") {
        println!("üìÑ Running scenario from: {}", scenario_path);
        run_scenario_benchmark(scenario_path, benchmark_config, &progress_writer).await?
    } else if let Some(suite_dir) = matches.get_one::<String>("suite") {
        println!("üìÅ Running benchmark suite from: {}", suite_dir);
        run_benchmark_suite(suite_dir, benchmark_config, &progress_writer).await?
    } else {
        // Traditional benchmark selection
        let benchmark_selection = determine_benchmark_selection(&matches);

        println!(
            "‚öôÔ∏è  Configuration: {}",
            if matches.get_flag("quick") {
                "Quick Test"
            } else if matches.get_flag("stress") {
                "Stress Test"
            } else {
                "Standard"
            }
        );

        run_selected_benchmarks(benchmark_config, benchmark_selection, &progress_writer).await?
    };

    // Generate reports unless disabled
    if !matches.get_flag("no-reports") {
        progress_writer.log_phase(BenchmarkPhase::ReportGeneration)?;
        progress_writer.log_progress(0.95)?;
        generate_reports(&results, &output_dir).await?;
        progress_writer.log_progress(1.0)?;
    } else {
        // Just save JSON results in data subfolder
        let json_path = output_dir.join("data").join("benchmark_results.json");
        let json_content = serde_json::to_string_pretty(&results)?;
        std::fs::write(&json_path, json_content)?;
        println!("üíæ Results saved to: {}", json_path.display());
    }

    let total_time = start_time.elapsed();

    // Print summary
    print_benchmark_summary(&results, total_time);

    progress_writer.log_phase(BenchmarkPhase::Complete)?;
    progress_writer.log_progress(1.0)?;

    Ok(())
}

/// Benchmark configuration structure
#[derive(Debug, Clone)]
struct BenchmarkConfig {
    quick_mode: bool,
    stress_mode: bool,
    _output_directory: PathBuf,
    // This would include all the configuration options from the benchmark modules
}

/// Which benchmarks to run
#[derive(Debug, Clone)]
struct BenchmarkSelection {
    run_scalability: bool,
    run_llm: bool,
    run_knowledge_graph: bool,
    run_evolution: bool,
    run_gpu_evolution: bool,
    run_gpu_knowledge_graph: bool,
    run_gpu_streaming: bool,
}

/// Benchmark results placeholder
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct BenchmarkResults {
    timestamp: u64,
    system_info: SystemInfo,
    scalability_results: Option<ScalabilityResults>,
    llm_results: Option<LlmResults>,
    knowledge_graph_results: Option<KnowledgeGraphResults>,
    evolution_results: Option<EvolutionResults>,
    gpu_evolution_results: Option<GpuEvolutionResults>,
    gpu_knowledge_graph_results: Option<GpuKnowledgeGraphResults>,
    gpu_streaming_results: Option<GpuStreamingResults>,
    summary: BenchmarkSummary,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct SystemInfo {
    gpu_name: String,
    gpu_memory_gb: f64,
    cuda_version: String,
    driver_version: String,
    system_memory_gb: f64,
    cpu_info: String,
    timestamp: String,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct ScalabilityResults {
    max_agents: usize,
    peak_agents_per_second: f64,
    memory_efficiency: f64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct LlmResults {
    max_agents_with_llm: usize,
    inference_throughput: f64,
    recommended_batch_size: usize,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct KnowledgeGraphResults {
    max_nodes: usize,
    query_throughput: f64,
    construction_rate: f64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct EvolutionResults {
    generations_per_second: f64,
    linear_scaling_limit: usize,
    parallel_efficiency: f64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct GpuEvolutionResults {
    max_population_size: usize,
    mutations_per_second: f64,
    generations_per_second: f64,
    genome_sizes_tested: Vec<usize>,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct GpuKnowledgeGraphResults {
    max_nodes: usize,
    max_edges: usize,
    query_latency_us: f64,
    build_time_ms: f64,
    queries_per_second: f64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct GpuStreamingResults {
    max_throughput_gbps: f64,
    avg_latency_ms: f64,
    compression_ratio: f64,
    data_sizes_tested_mb: Vec<usize>,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct BenchmarkSummary {
    max_agents_spawned: usize,
    max_agents_with_llm: usize,
    max_knowledge_graph_nodes: usize,
    evolution_performance_score: f64,
    gpu_max_population: usize,
    gpu_max_knowledge_nodes: usize,
    gpu_streaming_throughput_gbps: f64,
    overall_performance_rating: String,
    recommendations: Vec<String>,
}

/// Load benchmark configuration from file or create default
fn load_benchmark_config(matches: &ArgMatches) -> Result<BenchmarkConfig> {
    let config = BenchmarkConfig {
        quick_mode: matches.get_flag("quick"),
        stress_mode: matches.get_flag("stress"),
        _output_directory: PathBuf::from(matches.get_one::<String>("output")?),
    };

    // If config file specified, load it
    if let Some(config_path) = matches.get_one::<String>("config") {
        println!("üìã Loading configuration from: {}", config_path);
        // In a real implementation, would load and merge config from file
    }

    Ok(config)
}

/// Determine which benchmarks to run based on command line arguments
fn determine_benchmark_selection(matches: &ArgMatches) -> BenchmarkSelection {
    // Check if scenario or suite is specified
    if matches.contains_id("scenario") || matches.contains_id("suite") {
        // Will be handled separately in main
        return BenchmarkSelection {
            run_scalability: false,
            run_llm: false,
            run_knowledge_graph: false,
            run_evolution: false,
            run_gpu_evolution: false,
            run_gpu_knowledge_graph: false,
            run_gpu_streaming: false,
        };
    }

    // Check if GPU-all flag is set
    let gpu_all = matches.get_flag("gpu-all");
    let include_gpu = matches.get_flag("include-gpu");

    // If any specific benchmark is selected, run only those
    let any_specific = matches.get_flag("scalability-only")
        || matches.get_flag("llm-only")
        || matches.get_flag("knowledge-graph-only")
        || matches.get_flag("evolution-only")
        || matches.get_flag("gpu-evolution-only")
        || matches.get_flag("gpu-knowledge-only")
        || matches.get_flag("gpu-streaming-only")
        || gpu_all;

    if any_specific {
        BenchmarkSelection {
            run_scalability: matches.get_flag("scalability-only"),
            run_llm: matches.get_flag("llm-only"),
            run_knowledge_graph: matches.get_flag("knowledge-graph-only"),
            run_evolution: matches.get_flag("evolution-only"),
            run_gpu_evolution: matches.get_flag("gpu-evolution-only") || gpu_all,
            run_gpu_knowledge_graph: matches.get_flag("gpu-knowledge-only") || gpu_all,
            run_gpu_streaming: matches.get_flag("gpu-streaming-only") || gpu_all,
        }
    } else {
        // Run all benchmarks (include GPU if requested)
        BenchmarkSelection {
            run_scalability: true,
            run_llm: true,
            run_knowledge_graph: true,
            run_evolution: true,
            run_gpu_evolution: include_gpu,
            run_gpu_knowledge_graph: include_gpu,
            run_gpu_streaming: include_gpu,
        }
    }
}

/// Run selected benchmarks
async fn run_selected_benchmarks(
    config: BenchmarkConfig,
    selection: BenchmarkSelection,
    progress_writer: &ProgressWriter,
) -> Result<BenchmarkResults> {
    // Gather system information
    progress_writer.log_phase(BenchmarkPhase::SystemCheck)?;
    let system_info = gather_system_info(&progress_writer).await?;

    let mut scalability_results = None;
    let mut llm_results = None;
    let mut knowledge_graph_results = None;
    let mut evolution_results = None;
    let mut gpu_evolution_results = None;
    let mut gpu_knowledge_graph_results = None;
    let mut gpu_streaming_results = None;

    // Run scalability benchmarks
    if selection.run_scalability {
        println!("\nüîß Running Scalability Benchmarks...");
        progress_writer.log_phase(BenchmarkPhase::ScalabilityTests)?;
        progress_writer.log_progress(0.1)?;
        scalability_results = Some(run_scalability_benchmarks(&config, &progress_writer).await?);
        progress_writer.log_progress(0.25)?;
        println!("‚úÖ Scalability benchmarks completed");
    }

    // Run LLM benchmarks
    if selection.run_llm {
        println!("\nüß† Running LLM Integration Benchmarks...");
        progress_writer.log_phase(BenchmarkPhase::LlmTests)?;
        progress_writer.log_progress(0.3)?;
        llm_results = Some(run_llm_benchmarks(&config, &progress_writer).await?);
        progress_writer.log_progress(0.5)?;
        println!("‚úÖ LLM benchmarks completed");
    }

    // Run knowledge graph benchmarks
    if selection.run_knowledge_graph {
        println!("\nüï∏Ô∏è  Running Knowledge Graph Benchmarks...");
        progress_writer.log_phase(BenchmarkPhase::KnowledgeGraphTests)?;
        progress_writer.log_progress(0.55)?;
        knowledge_graph_results =
            Some(run_knowledge_graph_benchmarks(&config, &progress_writer).await?);
        progress_writer.log_progress(0.7)?;
        println!("‚úÖ Knowledge graph benchmarks completed");
    }

    // Run evolution benchmarks
    if selection.run_evolution {
        println!("\nüß¨ Running Evolution Strategy Benchmarks...");
        progress_writer.log_phase(BenchmarkPhase::EvolutionTests)?;
        progress_writer.log_progress(0.75)?;
        evolution_results = Some(run_evolution_benchmarks(&config, &progress_writer).await?);
        progress_writer.log_progress(0.9)?;
        println!("‚úÖ Evolution benchmarks completed");
    }

    // Run GPU Evolution benchmarks
    if selection.run_gpu_evolution {
        println!("\nüß¨‚ö° Running GPU Evolution Benchmarks...");
        progress_writer.log_phase(BenchmarkPhase::EvolutionTests)?;
        progress_writer.log_progress(0.91)?;
        gpu_evolution_results =
            Some(run_gpu_evolution_benchmarks(&config, &progress_writer).await?);
        progress_writer.log_progress(0.93)?;
        println!("‚úÖ GPU Evolution benchmarks completed");
    }

    // Run GPU Knowledge Graph benchmarks
    if selection.run_gpu_knowledge_graph {
        println!("\nüï∏Ô∏è‚ö° Running GPU Knowledge Graph Benchmarks...");
        progress_writer.log_phase(BenchmarkPhase::KnowledgeGraphTests)?;
        progress_writer.log_progress(0.94)?;
        gpu_knowledge_graph_results =
            Some(run_gpu_knowledge_graph_benchmarks(&config, &progress_writer).await?);
        progress_writer.log_progress(0.96)?;
        println!("‚úÖ GPU Knowledge Graph benchmarks completed");
    }

    // Run GPU Streaming benchmarks
    if selection.run_gpu_streaming {
        println!("\nüåä‚ö° Running GPU Streaming Benchmarks...");
        progress_writer.log_phase(BenchmarkPhase::EvolutionTests)?; // Using Evolution phase as placeholder
        progress_writer.log_progress(0.97)?;
        gpu_streaming_results =
            Some(run_gpu_streaming_benchmarks(&config, &progress_writer).await?);
        progress_writer.log_progress(0.99)?;
        println!("‚úÖ GPU Streaming benchmarks completed");
    }

    // Generate summary
    let summary = generate_benchmark_summary(
        &scalability_results,
        &llm_results,
        &knowledge_graph_results,
        &evolution_results,
        &gpu_evolution_results,
        &gpu_knowledge_graph_results,
        &gpu_streaming_results,
    );

    Ok(BenchmarkResults {
        timestamp: std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)?
            .as_secs(),
        system_info,
        scalability_results,
        llm_results,
        knowledge_graph_results,
        evolution_results,
        gpu_evolution_results,
        gpu_knowledge_graph_results,
        gpu_streaming_results,
        summary,
    })
}

/// Gather system information
async fn gather_system_info(progress_writer: &ProgressWriter) -> Result<SystemInfo> {
    // In a real implementation, would query actual system info
    progress_writer.log("Detecting GPU properties...")?;
    let gpu_props = gpu_agents::get_gpu_device_properties(0)?;
    progress_writer.log_success(&format!("Found GPU: {}", gpu_props.name))?;

    Ok(SystemInfo {
        gpu_name: gpu_props.name,
        gpu_memory_gb: gpu_props.total_memory as f64 / (1024.0 * 1024.0 * 1024.0),
        cuda_version: "12.2".to_string(),
        driver_version: "535.86".to_string(),
        system_memory_gb: 64.0,
        cpu_info: "AMD Ryzen 9 7950X".to_string(),
        timestamp: chrono::Utc::now()
            .format("%Y-%m-%d %H:%M:%S UTC")
            .to_string(),
    })
}

/// Run scalability benchmarks
async fn run_scalability_benchmarks(
    config: &BenchmarkConfig,
    progress_writer: &ProgressWriter,
) -> Result<ScalabilityResults> {
    use gpu_agents::benchmarks::run_scalability_benchmark;

    println!("   Testing GPU agent scalability...");
    progress_writer.log("Initializing GPU swarms...")?;
    progress_writer.log("Testing agent spawn rates...")?;

    let results = run_scalability_benchmark(config.quick_mode, config.stress_mode).await?;

    Ok(ScalabilityResults {
        max_agents: results.max_agents,
        peak_agents_per_second: results.peak_agents_per_second,
        memory_efficiency: results.memory_efficiency,
    })
}

/// Run LLM benchmarks
async fn run_llm_benchmarks(
    config: &BenchmarkConfig,
    progress_writer: &ProgressWriter,
) -> Result<LlmResults> {
    use gpu_agents::benchmarks::run_llm_benchmark;

    println!("   Testing LLM integration...");
    progress_writer.log("Initializing LLM models...")?;
    progress_writer.log("Testing collective reasoning...")?;

    let results = run_llm_benchmark(config.quick_mode, config.stress_mode).await?;

    Ok(LlmResults {
        max_agents_with_llm: results.max_agents_with_llm,
        inference_throughput: results.inference_throughput,
        recommended_batch_size: results.recommended_batch_size,
    })
}

/// Run knowledge graph benchmarks
async fn run_knowledge_graph_benchmarks(
    config: &BenchmarkConfig,
    progress_writer: &ProgressWriter,
) -> Result<KnowledgeGraphResults> {
    use gpu_agents::benchmarks::run_knowledge_graph_benchmark;

    println!("   Testing knowledge graph scaling...");
    progress_writer.log("Building knowledge graph...")?;

    let results = run_knowledge_graph_benchmark(config.quick_mode, config.stress_mode).await?;

    Ok(KnowledgeGraphResults {
        max_nodes: results.max_nodes,
        query_throughput: results.query_throughput,
        construction_rate: results.construction_rate,
    })
}

/// Run evolution benchmarks
async fn run_evolution_benchmarks(
    config: &BenchmarkConfig,
    progress_writer: &ProgressWriter,
) -> Result<EvolutionResults> {
    use gpu_agents::benchmarks::run_simplified_evolution_benchmark;

    println!("   Testing evolution strategies...");
    progress_writer.log("Running evolution algorithms...")?;

    // CUDA memory allocation issues have been fixed!
    // Using simplified benchmark to bypass GpuSwarm integration issues
    println!("   ‚úÖ Running simplified evolution benchmarks with fixed CUDA memory allocation...");

    let results = run_simplified_evolution_benchmark(config.quick_mode, config.stress_mode).await?;

    // Log population results
    for result in &results.population_results {
        progress_writer.log(&format!(
            "Population {}: {:.1} generations/second",
            result.population_size, result.generations_per_second
        ))?;
    }

    Ok(EvolutionResults {
        generations_per_second: results.generations_per_second,
        linear_scaling_limit: results.linear_scaling_limit,
        parallel_efficiency: results.parallel_efficiency,
    })
}

/// Run GPU evolution benchmarks
async fn run_gpu_evolution_benchmarks(
    config: &BenchmarkConfig,
    progress_writer: &ProgressWriter,
) -> Result<GpuEvolutionResults> {
    use gpu_agents::benchmarks::{GpuEvolutionBenchmark, GpuEvolutionBenchmarkConfig};

    println!("   Testing GPU evolution with large populations...");
    progress_writer.log("Initializing GPU evolution engine...")?;

    let benchmark_config = if config.quick_mode {
        GpuEvolutionBenchmarkConfig {
            population_sizes: vec![10_000, 100_000],
            genome_sizes: vec![256, 1024],
            generations: 10,
            ..Default::default()
        }
    } else {
        GpuEvolutionBenchmarkConfig::default()
    };

    let mut benchmark = GpuEvolutionBenchmark::new(0, benchmark_config)?;
    let results = benchmark.run_all().await?;

    // Find best results
    let mut max_population = 0;
    let mut max_mutations_per_sec: f64 = 0.0;
    let mut max_gens_per_sec: f64 = 0.0;
    let mut genome_sizes = Vec::new();

    for result in results {
        progress_writer.log(&format!(
            "Population {}: {:.1} mutations/sec",
            result.population_size, result.mutations_per_second
        ))?;

        max_population = max_population.max(result.population_size);
        max_mutations_per_sec = max_mutations_per_sec.max(result.mutations_per_second);
        max_gens_per_sec = max_gens_per_sec.max(1000.0 / result.avg_generation_time_ms);
        if !genome_sizes.contains(&result.genome_size) {
            genome_sizes.push(result.genome_size);
        }
    }

    Ok(GpuEvolutionResults {
        max_population_size: max_population,
        mutations_per_second: max_mutations_per_sec,
        generations_per_second: max_gens_per_sec,
        genome_sizes_tested: genome_sizes,
    })
}

/// Run GPU knowledge graph benchmarks
async fn run_gpu_knowledge_graph_benchmarks(
    config: &BenchmarkConfig,
    progress_writer: &ProgressWriter,
) -> Result<GpuKnowledgeGraphResults> {
    use gpu_agents::benchmarks::{GpuKnowledgeGraphBenchmark, GpuKnowledgeGraphBenchmarkConfig};

    println!("   Testing GPU knowledge graph with CSR format...");
    progress_writer.log("Building GPU knowledge graphs...")?;

    let benchmark_config = if config.quick_mode {
        GpuKnowledgeGraphBenchmarkConfig {
            node_counts: vec![10_000, 100_000],
            avg_edges_per_node: vec![10.0, 50.0],
            num_queries: 100,
            ..Default::default()
        }
    } else {
        GpuKnowledgeGraphBenchmarkConfig::default()
    };

    let mut benchmark = GpuKnowledgeGraphBenchmark::new(0, benchmark_config)?;
    let results = benchmark.run_all().await?;

    // Find best results
    let mut max_nodes = 0;
    let mut max_edges = 0;
    let mut min_query_latency = f64::MAX;
    let mut min_build_time = f64::MAX;
    let mut max_qps: f64 = 0.0;

    for result in results {
        progress_writer.log(&format!(
            "Graph {}: {:.1} queries/sec",
            result.node_count, result.queries_per_second
        ))?;

        max_nodes = max_nodes.max(result.node_count);
        max_edges = max_edges.max(result.edge_count);
        min_query_latency = min_query_latency.min(result.query_latency_us as f64);
        min_build_time = min_build_time.min(result.build_time_ms);
        max_qps = max_qps.max(result.queries_per_second);
    }

    Ok(GpuKnowledgeGraphResults {
        max_nodes,
        max_edges,
        query_latency_us: min_query_latency,
        build_time_ms: min_build_time,
        queries_per_second: max_qps,
    })
}

/// Run GPU streaming benchmarks
async fn run_gpu_streaming_benchmarks(
    config: &BenchmarkConfig,
    progress_writer: &ProgressWriter,
) -> Result<GpuStreamingResults> {
    use gpu_agents::benchmarks::{GpuStreamingBenchmark, StreamingBenchmarkConfig};

    println!("   Testing GPU streaming pipelines...");
    progress_writer.log("Initializing GPU stream processors...")?;

    let benchmark_config = if config.quick_mode {
        StreamingBenchmarkConfig {
            data_sizes_mb: vec![10, 100],
            chunk_sizes: vec![1024 * 1024],
            num_streams: vec![4, 8],
            iterations: 5,
            ..Default::default()
        }
    } else {
        StreamingBenchmarkConfig::default()
    };

    let mut benchmark = GpuStreamingBenchmark::new(0, benchmark_config)?;
    let results = benchmark.run_all().await?;

    // Find best results
    let mut max_throughput: f64 = 0.0;
    let mut avg_latency = 0.0;
    let mut avg_compression = 0.0;
    let mut data_sizes = Vec::new();

    for result in &results {
        progress_writer.log(&format!(
            "Stream {}MB: {:.2} GB/s throughput",
            result.data_size_mb, result.throughput_gbps
        ))?;

        max_throughput = max_throughput.max(result.throughput_gbps);
        avg_latency += result.latency_ms;
        avg_compression += result.compression_ratio;
        if !data_sizes.contains(&result.data_size_mb) {
            data_sizes.push(result.data_size_mb);
        }
    }

    if !results.is_empty() {
        avg_latency /= results.len() as f64;
        avg_compression /= results.len() as f64;
    }

    Ok(GpuStreamingResults {
        max_throughput_gbps: max_throughput,
        avg_latency_ms: avg_latency,
        compression_ratio: avg_compression,
        data_sizes_tested_mb: data_sizes,
    })
}

/// Generate benchmark summary
fn generate_benchmark_summary(
    scalability: &Option<ScalabilityResults>,
    llm: &Option<LlmResults>,
    knowledge_graph: &Option<KnowledgeGraphResults>,
    evolution: &Option<EvolutionResults>,
    gpu_evolution: &Option<GpuEvolutionResults>,
    gpu_knowledge_graph: &Option<GpuKnowledgeGraphResults>,
    gpu_streaming: &Option<GpuStreamingResults>,
) -> BenchmarkSummary {
    let max_agents_spawned = scalability.as_ref().map(|s| s.max_agents).unwrap_or(0);
    let max_agents_with_llm = llm.as_ref().map(|l| l.max_agents_with_llm).unwrap_or(0);
    let max_knowledge_graph_nodes = knowledge_graph.as_ref().map(|k| k.max_nodes).unwrap_or(0);
    let evolution_performance_score = evolution
        .as_ref()
        .map(|e| e.generations_per_second)
        .unwrap_or(0.0);

    // GPU results
    let gpu_max_population = gpu_evolution
        .as_ref()
        .map(|g| g.max_population_size)
        .unwrap_or(0);
    let gpu_max_knowledge_nodes = gpu_knowledge_graph
        .as_ref()
        .map(|g| g.max_nodes)
        .unwrap_or(0);
    let gpu_streaming_throughput_gbps = gpu_streaming
        .as_ref()
        .map(|g| g.max_throughput_gbps)
        .unwrap_or(0.0);

    // Calculate overall performance rating including GPU results
    let performance_score = (max_agents_spawned as f64 / 10_000_000.0) * 0.2
        + (max_agents_with_llm as f64 / 100_000.0) * 0.15
        + (max_knowledge_graph_nodes as f64 / 1_000_000.0) * 0.1
        + (evolution_performance_score / 100.0) * 0.05
        + (gpu_max_population as f64 / 10_000_000.0) * 0.2
        + (gpu_max_knowledge_nodes as f64 / 10_000_000.0) * 0.15
        + (gpu_streaming_throughput_gbps / 10.0) * 0.15;

    let overall_performance_rating = match performance_score {
        x if x >= 0.8 => "Excellent",
        x if x >= 0.6 => "Good",
        x if x >= 0.4 => "Fair",
        _ => "Needs Optimization",
    }
    .to_string();

    let mut recommendations = Vec::new();

    if max_agents_spawned < 1_000_000 {
        recommendations.push("Consider optimizing memory allocation for larger swarms".to_string());
    }

    if max_agents_with_llm < 10_000 {
        recommendations
            .push("LLM integration could benefit from batch size optimization".to_string());
    }

    if gpu_max_population < 10_000_000 {
        recommendations.push(
            "GPU evolution could scale to larger populations with memory optimization".to_string(),
        );
    }

    if gpu_streaming_throughput_gbps < 10.0 {
        recommendations
            .push("GPU streaming pipeline could benefit from larger batch sizes".to_string());
    }

    BenchmarkSummary {
        max_agents_spawned,
        max_agents_with_llm,
        max_knowledge_graph_nodes,
        evolution_performance_score,
        gpu_max_population,
        gpu_max_knowledge_nodes,
        gpu_streaming_throughput_gbps,
        overall_performance_rating,
        recommendations,
    }
}

/// Generate comprehensive reports in organized subdirectories
///
/// Following rust.md conventions with clear structure and error handling
async fn generate_reports(results: &BenchmarkResults, output_dir: &PathBuf) -> Result<()> {
    println!("\nüìä Generating Benchmark Reports...");

    // Define report paths in organized subdirectories
    let reports_dir = output_dir.join("reports");
    let data_dir = output_dir.join("data");

    // Generate JSON report in data subfolder
    let json_path = data_dir.join("benchmark_results.json");
    let json_content = serde_json::to_string_pretty(results)?;
    std::fs::write(&json_path, json_content)?;
    println!("   ‚úÖ JSON report: {}", json_path.display());

    // Generate HTML report in reports subfolder
    let html_path = reports_dir.join("benchmark_report.html");
    generate_html_report(results, &html_path).await?;
    println!("   ‚úÖ HTML report: {}", html_path.display());

    // Generate Markdown report in reports subfolder
    let md_path = reports_dir.join("benchmark_report.md");
    generate_markdown_report(results, &md_path).await?;
    println!("   ‚úÖ Markdown report: {}", md_path.display());

    // Generate CSV summary in data subfolder
    let csv_path = data_dir.join("benchmark_summary.csv");
    generate_csv_summary(results, &csv_path).await?;
    println!("   ‚úÖ CSV summary: {}", csv_path.display());

    // Generate metadata file with run information
    let metadata_path = data_dir.join("run_metadata.json");
    generate_run_metadata(&metadata_path).await?;
    println!("   ‚úÖ Run metadata: {}", metadata_path.display());

    Ok(())
}

/// Generate HTML report
async fn generate_html_report(results: &BenchmarkResults, output_path: &PathBuf) -> Result<()> {
    let html_content = format!(
        r#"
<!DOCTYPE html>
<html>
<head>
    <title>GPU Agents Benchmark Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .header {{ text-align: center; border-bottom: 2px solid #333; padding-bottom: 20px; }}
        .section {{ margin: 30px 0; }}
        .metric {{ background: #f5f5f5; padding: 15px; margin: 10px 0; border-left: 4px solid #007acc; }}
        .rating {{ padding: 10px; border-radius: 5px; text-align: center; font-weight: bold; }}
        .excellent {{ background: #d4edda; color: #155724; }}
        .good {{ background: #fff3cd; color: #856404; }}
        .fair {{ background: #f8d7da; color: #721c24; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>üöÄ GPU Agents Benchmark Report</h1>
        <p>Generated on {}</p>
    </div>
    
    <div class="section">
        <h2>üìã Executive Summary</h2>
        <div class="metric">
            <strong>Max Agents Spawned:</strong> {}
        </div>
        <div class="metric">
            <strong>Max Agents with LLM:</strong> {}
        </div>
        <div class="metric">
            <strong>Max Knowledge Graph Nodes:</strong> {}
        </div>
        <div class="metric">
            <strong>Evolution Performance Score:</strong> {:.2}
        </div>
        <div class="rating {}">
            Overall Performance Rating: {}
        </div>
    </div>
    
    <div class="section">
        <h2>üíª System Information</h2>
        <div class="metric">
            <strong>GPU:</strong> {} ({:.1} GB)<br>
            <strong>CUDA:</strong> {}<br>
            <strong>System RAM:</strong> {:.1} GB<br>
            <strong>CPU:</strong> {}
        </div>
    </div>
    
    {}
    
</body>
</html>
"#,
        chrono::Utc::now().format("%Y-%m-%d %H:%M:%S UTC"),
        results.summary.max_agents_spawned,
        results.summary.max_agents_with_llm,
        results.summary.max_knowledge_graph_nodes,
        results.summary.evolution_performance_score,
        get_rating_class(&results.summary.overall_performance_rating),
        results.summary.overall_performance_rating,
        results.system_info.gpu_name,
        results.system_info.gpu_memory_gb,
        results.system_info.cuda_version,
        results.system_info.system_memory_gb,
        results.system_info.cpu_info,
        generate_recommendations_html(&results.summary.recommendations)
    );

    std::fs::write(output_path, html_content)?;
    Ok(())
}

/// Generate Markdown report
async fn generate_markdown_report(results: &BenchmarkResults, output_path: &PathBuf) -> Result<()> {
    let markdown_content = format!(
        r#"# üöÄ GPU Agents Benchmark Report

Generated on: {}

## üìã Executive Summary

- **Max Agents Spawned:** {}
- **Max Agents with LLM:** {}
- **Max Knowledge Graph Nodes:** {}
- **Evolution Performance Score:** {:.2}
- **Overall Performance Rating:** {}

## üíª System Information

- **GPU:** {} ({:.1} GB VRAM)
- **CUDA Version:** {}
- **System RAM:** {:.1} GB
- **CPU:** {}
- **Test Date:** {}

## üéØ Recommendations

{}

---

*Report generated by GPU Agents Benchmark Suite*
"#,
        chrono::Utc::now().format("%Y-%m-%d %H:%M:%S UTC"),
        results.summary.max_agents_spawned,
        results.summary.max_agents_with_llm,
        results.summary.max_knowledge_graph_nodes,
        results.summary.evolution_performance_score,
        results.summary.overall_performance_rating,
        results.system_info.gpu_name,
        results.system_info.gpu_memory_gb,
        results.system_info.cuda_version,
        results.system_info.system_memory_gb,
        results.system_info.cpu_info,
        results.system_info.timestamp,
        results.summary.recommendations.join("\n- ")
    );

    std::fs::write(output_path, markdown_content)?;
    Ok(())
}

/// Generate CSV summary
async fn generate_csv_summary(results: &BenchmarkResults, output_path: &PathBuf) -> Result<()> {
    let csv_content = format!(
        "Metric,Value,Unit
Max Agents Spawned,{},agents
Max Agents with LLM,{},agents
Max Knowledge Graph Nodes,{},nodes
Evolution Performance Score,{:.2},score
Overall Performance Rating,{},rating
GPU Name,{},
GPU Memory,{:.1},GB
CUDA Version,{},
System RAM,{:.1},GB
",
        results.summary.max_agents_spawned,
        results.summary.max_agents_with_llm,
        results.summary.max_knowledge_graph_nodes,
        results.summary.evolution_performance_score,
        results.summary.overall_performance_rating,
        results.system_info.gpu_name,
        results.system_info.gpu_memory_gb,
        results.system_info.cuda_version,
        results.system_info.system_memory_gb
    );

    std::fs::write(output_path, csv_content)?;
    Ok(())
}

/// Print benchmark summary to console
fn print_benchmark_summary(results: &BenchmarkResults, total_time: std::time::Duration) {
    println!("\nüèÅ Benchmark Complete!");
    println!("======================");
    println!(
        "üìä Max Agents Spawned: {}",
        results.summary.max_agents_spawned
    );
    println!(
        "üß† Max Agents with LLM: {}",
        results.summary.max_agents_with_llm
    );
    println!(
        "üï∏Ô∏è  Max Knowledge Graph Nodes: {}",
        results.summary.max_knowledge_graph_nodes
    );
    println!(
        "üß¨ Evolution Performance: {:.2}",
        results.summary.evolution_performance_score
    );

    // GPU results
    if results.summary.gpu_max_population > 0 {
        println!("\nüöÄ GPU Performance:");
        println!(
            "   üß¨ Max GPU Population: {}",
            results.summary.gpu_max_population
        );
        println!(
            "   üï∏Ô∏è  Max GPU Knowledge Nodes: {}",
            results.summary.gpu_max_knowledge_nodes
        );
        println!(
            "   üåä GPU Streaming Throughput: {:.2} GB/s",
            results.summary.gpu_streaming_throughput_gbps
        );
    }

    println!(
        "\n‚≠ê Overall Rating: {}",
        results.summary.overall_performance_rating
    );
    println!("‚è±Ô∏è  Total Time: {:.2}s", total_time.as_secs_f64());

    if !results.summary.recommendations.is_empty() {
        println!("\nüéØ Recommendations:");
        for rec in &results.summary.recommendations {
            println!("   ‚Ä¢ {}", rec);
        }
    }
}

/// Helper functions
fn get_rating_class(rating: &str) -> &'static str {
    match rating {
        "Excellent" => "excellent",
        "Good" => "good",
        "Fair" => "fair",
        _ => "fair",
    }
}

fn generate_recommendations_html(recommendations: &[String]) -> String {
    if recommendations.is_empty() {
        return String::new();
    }

    let rec_list = recommendations
        .iter()
        .map(|r| format!("<li>{}</li>", r))
        .collect::<Vec<_>>()
        .join("");

    format!(
        r#"
    <div class="section">
        <h2>üéØ Recommendations</h2>
        <ul>{}</ul>
    </div>
    "#,
        rec_list
    )
}

/// Scenario definition for running custom benchmark configurations
#[derive(Debug, Clone, Serialize, Deserialize)]
struct BenchmarkScenario {
    name: String,
    description: Option<String>,
    benchmarks: Vec<ScenarioBenchmark>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct ScenarioBenchmark {
    #[serde(rename = "type")]
    benchmark_type: String,
    enabled: bool,
    #[serde(default)]
    config: serde_json::Value,
}

/// Run a single scenario from a YAML/TOML file
async fn run_scenario_benchmark(
    scenario_path: &str,
    config: BenchmarkConfig,
    progress_writer: &ProgressWriter,
) -> Result<BenchmarkResults> {
    let path = Path::new(scenario_path);
    let content = std::fs::read_to_string(path)
        .with_context(|| format!("Failed to read scenario file: {}", scenario_path))?;

    let scenario: BenchmarkScenario = if path.extension().and_then(|s| s.to_str()) == Some("toml") {
        toml::from_str(&content)?
    } else {
        serde_yaml::from_str(&content)?
    };

    println!("üìã Running scenario: {}", scenario.name);
    if let Some(desc) = &scenario.description {
        println!("   {}", desc);
    }

    // Convert scenario benchmarks to selection
    let mut selection = BenchmarkSelection {
        run_scalability: false,
        run_llm: false,
        run_knowledge_graph: false,
        run_evolution: false,
        run_gpu_evolution: false,
        run_gpu_knowledge_graph: false,
        run_gpu_streaming: false,
    };

    for benchmark in &scenario.benchmarks {
        if benchmark.enabled {
            match benchmark.benchmark_type.as_str() {
                "scalability" => selection.run_scalability = true,
                "llm" => selection.run_llm = true,
                "knowledge_graph" => selection.run_knowledge_graph = true,
                "evolution" => selection.run_evolution = true,
                _ => println!("‚ö†Ô∏è  Unknown benchmark type: {}", benchmark.benchmark_type),
            }
        }
    }

    run_selected_benchmarks(config, selection, progress_writer).await
}

/// Run all scenarios in a directory
async fn run_benchmark_suite(
    suite_dir: &str,
    config: BenchmarkConfig,
    progress_writer: &ProgressWriter,
) -> Result<BenchmarkResults> {
    let suite_path = Path::new(suite_dir);
    if !suite_path.is_dir() {
        anyhow::bail!("Suite path is not a directory: {}", suite_dir);
    }

    let mut all_results = Vec::new();

    // Find all YAML and TOML files in the directory
    for entry in std::fs::read_dir(suite_path)? {
        let entry = entry?;
        let path = entry.path();

        if let Some(ext) = path.extension().and_then(|s| s.to_str()) {
            if ext == "yaml" || ext == "yml" || ext == "toml" {
                println!("\nüîÑ Running scenario: {}", path.display());
                let results =
                    run_scenario_benchmark(path.to_str().unwrap(), config.clone(), progress_writer)
                        .await?;
                all_results.push(results);
            }
        }
    }

    if all_results.is_empty() {
        anyhow::bail!("No scenario files found in directory: {}", suite_dir);
    }

    // Merge results (simplified - just return the last one for now)
    // In a real implementation, would properly merge all results
    Ok(all_results.into_iter().last().unwrap())
}

/// Create a timestamped output directory for benchmark reports
///
/// Creates directory structure: base_dir/YYYY-MM-DD_HH-MM-SS/
/// Following rust.md naming conventions with clear, descriptive names
fn create_timestamped_output_dir(base_dir: &PathBuf) -> Result<PathBuf> {
    // Generate timestamp in ISO-like format suitable for directory names
    let timestamp: DateTime<Local> = Local::now();
    let timestamp_str = timestamp.format("%Y-%m-%d_%H-%M-%S").to_string();

    // Create timestamped directory path
    let timestamped_dir = base_dir.join(timestamp_str);

    // Create the directory structure
    std::fs::create_dir_all(&timestamped_dir).with_context(|| {
        format!(
            "Failed to create timestamped output directory: {}",
            timestamped_dir.display()
        )
    })?;

    // Create subdirectories for organized report storage
    let subdirs = ["logs", "reports", "data"];
    for subdir in &subdirs {
        let subdir_path = timestamped_dir.join(subdir);
        std::fs::create_dir_all(&subdir_path)
            .with_context(|| format!("Failed to create subdirectory: {}", subdir_path.display()))?;
    }

    Ok(timestamped_dir)
}

/// Generate run metadata file containing benchmark execution details
///
/// Following rust.md conventions with proper error handling and documentation
async fn generate_run_metadata(output_path: &PathBuf) -> Result<()> {
    use std::collections::HashMap;

    let timestamp = Local::now();
    let mut metadata = HashMap::new();

    // Basic run information
    metadata.insert("run_timestamp".to_string(), timestamp.to_rfc3339());
    metadata.insert(
        "run_date".to_string(),
        timestamp.format("%Y-%m-%d").to_string(),
    );
    metadata.insert(
        "run_time".to_string(),
        timestamp.format("%H:%M:%S").to_string(),
    );
    metadata.insert("timezone".to_string(), timestamp.format("%z").to_string());

    // System information
    metadata.insert(
        "hostname".to_string(),
        std::env::var("HOSTNAME").unwrap_or_else(|_| "unknown".to_string()),
    );
    metadata.insert(
        "user".to_string(),
        std::env::var("USER").unwrap_or_else(|_| "unknown".to_string()),
    );
    metadata.insert(
        "working_directory".to_string(),
        std::env::current_dir()?.to_string_lossy().to_string(),
    );

    // Benchmark configuration
    metadata.insert("benchmark_version".to_string(), "1.0.0".to_string());
    metadata.insert(
        "rust_version".to_string(),
        std::env::var("RUSTC_VERSION").unwrap_or_else(|_| "unknown".to_string()),
    );

    // Write metadata as JSON
    let json_content = serde_json::to_string_pretty(&metadata)?;
    std::fs::write(output_path, json_content)?;

    Ok(())
}
