[package]
name = "horizon"
version.workspace = true
edition.workspace = true
authors.workspace = true
license.workspace = true
description = "Unified HPC-AI Platform - Single pane of glass for the entire stack"
build = "build.rs"

[lib]
name = "horizon_lib"
crate-type = ["staticlib", "cdylib", "rlib"]

[[bin]]
name = "horizon"
path = "src/main.rs"

[build-dependencies]
tauri-build.workspace = true

[dependencies]
tauri.workspace = true
tokio.workspace = true
async-trait.workspace = true
serde.workspace = true
serde_json.workspace = true
thiserror.workspace = true
anyhow.workspace = true
tracing.workspace = true
tracing-subscriber.workspace = true
uuid = { version = "1.11", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
sysinfo = "0.30"  # For system/GPU detection
hex = "0.4"  # For encoding merkle roots
dashmap = "6.1"  # Lock-free concurrent HashMap
rand = "0.8"  # Random number generation for ephemeral tokens

# Internal crate dependencies
hpc-channels.workspace = true
stratoswarm-cluster-mesh = { path = "../crates/cluster-mesh", optional = true }
rtx-distributed = { path = "../../08-rustytorch/crates/training/rtx-distributed", optional = true, default-features = false }
warp-core = { path = "../../05-warp/crates/warp-core", optional = true }
warp-format = { path = "../../05-warp/crates/warp-format", optional = true }
# vortex-core = { path = "../../09-vortex/crates/vortex-core", optional = true }

# GPU-accelerated Rust compilation (10x faster)
# Works on macOS (Metal) and Linux/Windows (CUDA)
rustg = { path = "../../00-rust/rustg", optional = true, default-features = false }

# SLAI GPU scheduling and multi-tenant job management
slai = { path = "../../04-slai", optional = true }

[features]
# Default: no embedded features (uses mock implementations)
default = []

# Embed stratoswarm cluster-mesh for real cluster management
# NOTE: Currently Linux-only due to procfs dependency in cluster-mesh
# On macOS/Windows, the mock implementation is used automatically
embedded-cluster = ["stratoswarm-cluster-mesh"]

# Embed rtx-distributed for real distributed training
# NOTE: Requires CUDA/NCCL - on non-CUDA systems, mock implementation is used
embedded-training = ["rtx-distributed"]

# Embed warp-core for real file transfers
# Works on all platforms - local and remote transfers
embedded-storage = ["warp-core", "warp-format"]

# Embed vortex for protocol transmutation (when available)
# embedded-vortex = ["vortex-core"]

# GPU-accelerated Rust compilation via rustg
# Provides 10x faster compilation in notebook cells
# macOS: Uses Metal GPU acceleration
# Linux/Windows: Uses CUDA GPU acceleration
gpu-compiler = ["rustg"]

# Embed SLAI for real GPU scheduling and multi-tenant job management
# Provides FairShareScheduler, GpuDetector, and Tenant management
embedded-slai = ["slai"]

# Embed nebula for RDMA transport and ZK proofs
# NOTE: RDMA requires Linux with InfiniBand/RoCE
# On non-Linux systems, mock implementation is used
# embedded-nebula = ["nebula-rdma", "nebula-zk", "nebula-relay"]

# Embed all optional crates for full functionality
full = ["embedded-cluster", "embedded-training", "embedded-storage", "embedded-slai", "gpu-compiler"]

# Minimal build without embedded crates (uses mocks)
minimal = []

[profile.release]
# Optimize for size while maintaining reasonable performance
opt-level = "z"
lto = true
codegen-units = 1
strip = true
panic = "abort"

[profile.dev]
# Faster incremental builds
opt-level = 0
incremental = true
